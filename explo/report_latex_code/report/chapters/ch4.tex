\chapter{Conclusions and Discussion}\label{final}

We conducted a small-scale comparison to analyze SDP performance differences among 11 clustering-based unsupervised models and 5 typical supervised models. We made the first step towards investigating the impacts of the feature types of defect data on the performance of these methods. Since the defect data was highly imbalanced so we also used oversampling methods to balance the data.\newline
Our experimental results on 17 defect data indicate that not all clustering-based unsupervised models are worse than the supervised models.\newline
Firstly we are using accuracy performance indicator-\newline
If we compare supervised algorithms only, RF performed better over other algorithms. In unsupervised models , KMS model performed better over other models.\newline
If we see MCC performance of our models we find that RF in supervised model family and MBM in unsupervised model family performed better.\newline
But when we have an imbalanced data ,accuracy is not a good metric.
So we are making conclusion on the basis of f1-score.\newline
To sum up, on defect data we can say that Random Forest in supervised model family and DBSCAN in  unsupervised model family performed best on f1-score performance indicator.

